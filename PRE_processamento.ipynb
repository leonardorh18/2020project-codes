{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PRE processamento",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wbg2Q6XiHMr",
        "outputId": "3cc2f081-a210-4231-d772-48c0004d7c7d"
      },
      "source": [
        "import gensim\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk import download\n",
        "download('wordnet')\n",
        "download('averaged_perceptron_tagger')\n",
        "from nltk import WordNetLemmatizer, pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOi4qRyMw174"
      },
      "source": [
        "stops = pd.read_csv('/content/drive/My Drive/datasets/articles/monthPre/final/completo/preprocessedCompletoComBigrams.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cDqpwTHyn5P"
      },
      "source": [
        "mes1 = stops[stops.month == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deXF1j9Z1k53"
      },
      "source": [
        "news = stops[stops.source == 'huffpost']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuC-pnQAysb4"
      },
      "source": [
        "idx = 0 \n",
        "for doc in mes1.itertuples():\n",
        "  for word in doc.content.split(' '):\n",
        "    if word == 'george_floyd':\n",
        "      print(doc.link)\n",
        "      idx +=1\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlOOOy0axM7A"
      },
      "source": [
        "links = stops.link"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wum5n4SOxYt2"
      },
      "source": [
        "news = stops.content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIg0M4JKxcUm"
      },
      "source": [
        "links[500085]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co3NySKzxbK-"
      },
      "source": [
        "news[500085]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGQzXhpKkRiR"
      },
      "source": [
        "stops = pd.read_csv('/content/drive/My Drive/datasets/articles/share/stops/stops.txt')\n",
        "stopsList = stops.document\n",
        "stops = [word for word in stopsList]\n",
        "for w in STOPWORDS:\n",
        "  stops.append(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEXwlX0LiaEQ"
      },
      "source": [
        "class PreProcessing(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.__wordNet = WordNetLemmatizer()\n",
        "       \n",
        "    \n",
        "    def __clearHTML(self, doc):\n",
        "        soup = BeautifulSoup(doc, 'lxml')\n",
        "        for tag in soup.find_all('code'):\n",
        "            tag.decompose()\n",
        "        return self.__removeCode(soup.get_text(), repeat = 4)\n",
        "    \n",
        "    def __getPOS(self, token):\n",
        "        tag = pos_tag([token])[0][1][0].upper()\n",
        "        tagDict = {\n",
        "            \"J\": wordnet.ADJ,\n",
        "            \"N\": wordnet.NOUN,\n",
        "            \"V\": wordnet.VERB,\n",
        "            \"R\": wordnet.ADV\n",
        "        }\n",
        "        return tagDict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "    def __removeCode(self, content, repeat = 1):\n",
        "        for i in range(0,repeat):\n",
        "            #AS VEZES, quando executa uma vez sobra algum lixo ainda, por isso o repeat\n",
        "            content = re.sub(\"\\w*\\.\\w*\\.\\w*\", \"\", content) # retira palavras seguidas de pontos nesse formato ex: place.googletag.cmd\n",
        "            content = re.sub(r\"\\{[^{}]*\\}\", \"\", content) #retira palavras entre { }\n",
        "            content = re.sub(r\"\\w*\\.\\w+\\([^()]*\\)\", \"\", content) ##retira palavras com ponto que seguem de ( ) ex: document.function(...)\n",
        "            content = re.sub(r\"\\w+\\([^()]*\\)\", \"\", content) ##retira palavras que seguem de ( ) ex: function(...)\n",
        "        return content\n",
        "\n",
        "    def __applyNLP(self, doc):\n",
        "        newDoc = []\n",
        "        for token in simple_preprocess(doc, deacc=True):\n",
        "          if self.__getPOS(token) == 'v': ## verbo tem retorno do POS como 'v', caso queira remover outro POS é só acrescentar a sigla na lista\n",
        "            continue\n",
        "          #print(self.__getPOS(token))\n",
        "          token = self.__wordNet.lemmatize(token, pos=self.__getPOS(token))\n",
        "          if token not in stops and len(token) > 2:\n",
        "              newDoc.append(token)\n",
        "        return newDoc\n",
        "    \n",
        "    def __cleaning(self, corpus):\n",
        "        cleanCorpus = []\n",
        "        for doc in corpus:\n",
        "            doc = self.__clearHTML(doc)\n",
        "            doc = self.__applyNLP(doc)\n",
        "            cleanCorpus.append(doc)\n",
        "        return cleanCorpus\n",
        "    \n",
        "    def __makeBigrams(self, corpus):\n",
        "        # Train Phrases model\n",
        "        bigramModel.add_vocab(corpus)\n",
        "        # Make bi-grams\n",
        "        \n",
        "        preProcessedCorpus = []\n",
        "        for doc in corpus:\n",
        "          #doc = bigramModel[doc]\n",
        "          if self.__join:\n",
        "              doc = ' '.join(doc)\n",
        "          preProcessedCorpus.append(doc)\n",
        "        \n",
        "        return  corpus, bigramModel\n",
        "\n",
        "    def execute(self, corpus, bModel, join=False):\n",
        "        self.__join = join\n",
        "        self.__bigramModel = bModel\n",
        "        return self.__makeBigrams(self.__cleaning(corpus))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaN4-UjWlkxU"
      },
      "source": [
        "def remove(content):\n",
        "  newDocs = []\n",
        "  for doc in content:\n",
        "    docLine = ''\n",
        "    aux = []\n",
        "    for word in doc.split(','):\n",
        "      word = \"\".join(c  for c in word if c not in [\"'\", '\"', '[', ']', \" \"] )\n",
        "      #docLine += word+' '\n",
        "      aux.append(word)\n",
        "    newDocs.append(aux)\n",
        "  return newDocs\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzarMIxzm3wd"
      },
      "source": [
        "nc = remove(test.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7S-SCriic_I"
      },
      "source": [
        "bigramModel = Phrases()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAhHcKXDkgec"
      },
      "source": [
        "preProcessing = PreProcessing()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsVS_rVvlHQ0"
      },
      "source": [
        "bigramModel = Phrases()\n",
        "bigramModel = bigramModel.load('/content/drive/My Drive/datasets/articles/bmodel/bmodel.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg1PYPKFldmj"
      },
      "source": [
        "test = pd.read_csv('/content/drive/My Drive/datasets/articles/monthPre/monthpre2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW6O-m1eiSx5"
      },
      "source": [
        "#dataset divided by month\n",
        "for i in range(1,13):\n",
        "  print('Mes:', i)\n",
        "  dfatual  = pd.read_csv('/content/drive/My Drive/datasets/articles/datasetsDivided/month{}.csv'.format(i))\n",
        "  dfatual = dfatual[dfatual['content'].notna()]\n",
        "  \n",
        "  newCorpus, bigramModel = preProcessing.execute(dfatual.content, bigramModel, join = True )\n",
        "  #print(newCorpus[0])\n",
        "  print('Salvando modelo de bigrams...')\n",
        "  bigramModel.save(fname_or_handle= '/content/drive/My Drive/datasets/articles/bmodel/bmodel.bin', )\n",
        "\n",
        "  dfatual['content'] = newCorpus\n",
        "  print('Salvando csv')\n",
        "  dfatual.to_csv('/content/drive/My Drive/datasets/articles/monthPre/monthpre{}.csv'.format(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkzePdlH0RQp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjkM1uQlS20"
      },
      "source": [
        "de sexta dia 19\n",
        "Mes: 1\n",
        "Salvando modelo de bigrams...\n",
        "Salvando csv\n",
        "Mes: 2\n",
        "Salvando modelo de bigrams...\n",
        "Salvando csv\n",
        "Mes: 3\n",
        "Salvando modelo de bigrams...\n",
        "Salvando csv\n",
        "Mes: 4"
      ]
    }
  ]
}